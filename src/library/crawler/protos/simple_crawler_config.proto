package ycrawler;


message SimpleCrawlerConfig {
    message State {
        optional string directory                 = 1 [default = "simple_crawler_state"];
        optional string queued_urls_file_name     = 2 [default = "queued_urls.tsv"];
        optional string failed_urls_file_name     = 3 [default = "failed_urls.tsv"];
        optional string processed_urls_file_name  = 4 [default = "processed_urls.tsv"];
        optional string config_file_name          = 5 [default = "config.pb.bin"];
    };
    message Documents {
        optional string documents_directory       = 1 [default = "docs"];
        optional string documents_data_directory  = 2 [default = "docs_data"];
        optional string url_to_id_file_name       = 3 [default = "url_to_id.tsv"];
        optional string web_graph_file_name       = 4 [default = "web_graph.txt"];
    };

    optional uint32 threads                        = 1 [default = 4];
    repeated string urls_seed                      = 2;
    optional uint32 tries_limit                    = 3 [default = 10];
    // save crawler state and documents data after each N jobs
    optional uint32 backup_frequency               = 9 [default = 1000];

    // for debugging purpose
    optional bool processed_urls_is_limited        = 6 [default = true];
    optional uint32 processed_urls_limit           = 7 [default = 30];

    optional string logs_directory                 = 8 [default = "logs"];

    optional State state                           = 4;
    optional Documents documents                   = 5;
};
